import random
import math
import copy
import os

import numpy
import scipy.optimize
import multiprocessing
import numpy as np
import smt.surrogate_models as smt
import smt.sampling_methods as smt_sampling
import prettytable

from sklearn.ensemble import RandomForestClassifier
from sas.core.discipline import DesignCompetence, Discipline
from sas.kadmos_interface.cpacs import PortableCpacs
from typing import Union


class SurrogateModel(DesignCompetence):
    """ Surrogate Model Engine of the Surrogate Advisory System.

        Contains options to train, validate and execute Surrogate Models from the SMT library.

        Contains overview of available models in SMT and their characteristics. Can be updated for added functionalities to
        SMT. Please adhere to specific format, as it drives the hyperparameter optimization. Important is the difference
        between discrete and continuous variables.

        Has a specific structure:
        {MODEL NAME -> {'function': lambda to SMT object,
                         'hyperparameters': 'nameContinuousParameter': {'min': xxx,
                                                                       'max': xxx},
                                            'nameDiscreteParameter': [val1, val2, val3]}} """
    available_models = {
        'RBF': {'function': lambda **kwargs: smt.RBF(print_global=False, **kwargs),
                # Also 	Regularization coeff. Maybe include later
                'hyperparameters': {'d0': {'min': 0.5,
                                           'max': 10,
                                           'nom': 1},
                                    'poly_degree': [-1, 0, 1]}},
        'IDW': {'function': lambda **kwargs: smt.IDW(print_global=False, **kwargs),
                'hyperparameters': {'p': {'min': 0.5,
                                          'max': 10,
                                          'nom': 2.5}}},
        'LS': {'function': lambda **kwargs: smt.LS(print_global=False, **kwargs),
               'hyperparameters': {}},
        'QP': {'function': lambda **kwargs: smt.QP(print_global=False, **kwargs),
               'hyperparameters': {}},
        'KRG': {'function': lambda **kwargs: smt.KRG(print_global=False, **kwargs),
                'hyperparameters': {'poly': ['constant', 'linear', 'quadratic'],
                                    'corr': ['abs_exp', 'squar_exp', 'matern52', 'matern32']}},
        'KPLS': {'function': lambda **kwargs: smt.KPLS(print_global=False, **kwargs),
                 'hyperparameters': {'poly': ['constant', 'linear', 'quadratic'],
                                     'corr': ['abs_exp', 'squar_exp']}},
        'KPLSK': {'function': lambda **kwargs: smt.KPLSK(print_global=False, **kwargs),
                  'hyperparameters': {'poly': ['constant', 'linear', 'quadratic'],
                                      'corr': ['squar_exp']}},
        'GEKPLS': {'function': lambda **kwargs: smt.GEKPLS(print_global=True,
                                                           xlimits=np.array([[-10, 0], [-10, 0]]),
                                                           n_comp=2, **kwargs),
                   'hyperparameters': {'poly': ['constant', 'linear', 'quadratic'],
                                       'corr': ['abs_exp', 'squar_exp']}}
        # 'categorical_kernel': ['gower', 'homoscedastic_gaussian_matrix_kernel',
        # 'full_gaussian_matrix_kernel']

    }

    is_surrogate = True

    def __init__(self,
                 disciplines: Union[Discipline, list[Discipline]] = None,
                 input_variables=None,
                 output_variables=None,
                 converged=False,
                 all_inputs=None):

        if isinstance(disciplines, Discipline):
            kadmos_uid = f"{disciplines.uid}_SM"
            disciplines = [disciplines]
        else:
            if len(disciplines) > 1:
                kadmos_uid = "".join(tuple([discipline.id for discipline in disciplines]))
            else:
                kadmos_uid = f"{disciplines[0].uid}_SM"
        # Set information for storage in CMDOWS later.

        description = "Surrogate Model generated by SAS"
        command = ""
        version = 1.0
        status = "Testing"
        super(SurrogateModel, self).__init__(kadmos_uid=kadmos_uid,
                                             id=kadmos_uid,
                                             description=description,
                                             command=command,
                                             version=version,
                                             status=status)
        self.converged = converged
        self.input_variables = None
        self.output_variables = None
        self.input_data = None
        self.output_data = None
        self.n_total_samples = 0
        self.hidden_constraints = []
        self.validation_metric = 'RMSPE'

        # SMT works with numpy. Following properties handle this:
        self.all_input_samples = None
        self.all_output_samples = None
        self.input_samples_indices = None
        self.output_samples_indices = None

        self.surrogate = None
        self.error = None

        # Processing of hidden constraints
        self.valid_input_samples = None
        self.valid_output_samples = None
        self.n_valid_samples = None
        self.invalid_samples = []
        self.hidden_constraint_output_indices = None

        if disciplines:
            self.add_disciplines(disciplines, input_variables, output_variables)
        else:
            self.disciplines = None

        if all_inputs is not None:
            self.all_input_variables = all_inputs
        else:
            self.all_input_variables = self.input_variables

    def build(self, mode: str = 'automatic',
              selection: Union[str, list[str]] = 'all',
              model_type: str = 'KRG',
              model_options: dict = None,
              training_set: str = 'full'):

        if len(self.hidden_constraints) > 0:
            self.train_hidden_constraints()

        if mode == 'automatic':
            self._auto_select_surrogate(selection, training_set)
        elif mode == 'optimize_config':
            self._auto_select_surrogate(model_type, training_set)
        elif mode == 'fixed_type':
            self._train_fixed_type(model_type=model_type, model_options=model_options, training_set=training_set)
        else:
            raise AssertionError("Please provide valid mode: 'automatic', 'optimize_config', or 'fixed_type'")

    def validate(self, sur_model: smt = None, method='k-fold', **kwargs):
        """ Validate a surrogate model using a range of techniques

        :param sur_model: SMT object of a surrogate model
        :param method: 'k-fold' or 'split-sample'
        :param kwargs: split_size for split-sample, k_folds for k-folds
        :return: RMSE or similar
        """
        if sur_model is None:
            sur_model = copy.deepcopy(self.surrogate)

        if method == 'k-fold':
            errors = self._k_fold_validation(sur_model, **kwargs)
            error = sum(errors) / len(errors)
        elif method == 'leave-one-out':
            errors = self._k_fold_validation(sur_model, k_folds=self.n_valid_samples)
            error = sum(errors) / len(errors)
        elif method == 'split-sample':
            error = self._split_sample_validation(sur_model, **kwargs)
        else:
            AssertionError('Please provide a valid validation method')
            return

        return error

    def add_disciplines(self, disciplines: Union[Discipline, list[Discipline]], input_variables, output_variables):
        """ Add disciplines to the surrogate model object.
        The order of the disciplines in the provided list, is important!

        :param input_variables:
        :param output_variables:
        :param disciplines: Disciplines to surrogate model.
        :return:
        """
        if isinstance(disciplines, Discipline):
            disciplines = [disciplines]

        self.input_variables = input_variables
        self.output_variables = output_variables

        self.disciplines = disciplines

        self.update_data()

    def get_design_variables(self):
        design_variables = []
        for var, idx in self.input_samples_indices.items():
            if not min(self.valid_input_samples[:, idx]) - max(self.valid_input_samples[:, idx]) == 0:
                design_variables.append(var)

        return design_variables

    def update_data(self):
        """Retreive all the data from the connected disciplines.

        Consistency between samples for multiple disciplines is ensured by tracking the run_id and sample number in run.
        """
        self._find_in_and_output_providers()
        data = {}
        discipline = self.disciplines[0]
        initial_data = discipline.get_batched_samples()

        for discipline in self.disciplines:
            data[discipline] = discipline.get_batched_samples()

        sample_map = {}
        sample_counter = 0
        for run_id in initial_data:
            initial_data[run_id] = dict(sorted(initial_data[run_id].items()))

            for sample_in_run in initial_data[run_id]:
                missing_sample = False
                for discipline, data_disc in data.items():
                    if sample_in_run not in data_disc[run_id]:
                        missing_sample = True
                        break
                if missing_sample:
                    continue

                sample_map[sample_counter] = dict(run_id=run_id,
                                                  sample_in_run=sample_in_run)
                sample_counter += 1

        input_data = {}
        output_data = {}
        for input_var, discipline in self.input_providers.items():
            input_var_data = []
            for sample_number, sample_location in sample_map.items():
                run_id = sample_location['run_id']
                sample_in_run = sample_location['sample_in_run']
                input_var_data.append(data[discipline][run_id][sample_in_run]['input'][input_var])
            input_data[input_var] = input_var_data

        for output_var, discipline in self.output_providers.items():
            output_var_data = []
            for sample_number, sample_location in sample_map.items():
                run_id = sample_location['run_id']
                sample_in_run = sample_location['sample_in_run']
                output_var_data.append(data[discipline][run_id][sample_in_run]['output'][output_var])
            output_data[output_var] = output_var_data

        if self.converged:
            input_data, output_data, sample_map = self._filter_converged_data(input_data, output_data, sample_map)

        self.input_data = input_data
        self.output_data = output_data
        self.n_total_samples = len(sample_map)
        self._process_data()

        if self.n_total_samples > 0:
            self.update_hidden_constraints()
            self._calculate_non_linearity()

    def _filter_converged_data(self, input_data, output_data, old_sample_map):
        unique_input_samples = []
        converged_sample_numbers = []
        new_sample_map = {}
        for sample_number in old_sample_map:
            input_sample = {}
            for var in input_data:
                input_sample[var] = input_data[var][sample_number]

            if input_sample not in unique_input_samples:
                unique_input_samples.append(input_sample)
                converged_sample_numbers.append(sample_number)
                new_sample_map[len(unique_input_samples) - 1] = dict(run_id=old_sample_map[sample_number]['run_id'],
                                                                     sample_in_run=old_sample_map[sample_number][
                                                                         'sample_in_run'])
            else:
                idx_unique_sample = unique_input_samples.index(input_sample)
                converged_sample_numbers[idx_unique_sample] = sample_number
                new_sample_map[idx_unique_sample] = dict(run_id=old_sample_map[sample_number]['run_id'],
                                                         sample_in_run=old_sample_map[sample_number]['sample_in_run'])

        for var in input_data:
            input_data[var] = [input_data[var][idx] for idx in converged_sample_numbers]

        for var in output_data:
            output_data[var] = [output_data[var][idx] for idx in converged_sample_numbers]

        return input_data, output_data, new_sample_map

    def update_hidden_constraints(self):
        for discipline in self.disciplines:
            self.add_hidden_constraints(discipline.hidden_constraints)
        self._filter_hidden_constraints()

    def add_hidden_constraint(self, hidden_constraint, filter_data: bool = True):
        """Add a single hidden constraint to the surrogate model

        :param filter_data: Run a filtering run on the surrogate model's data
        :param hidden_constraint: HiddenConstraint
        """
        if not self.hidden_constraints:
            self.hidden_constraints = [hidden_constraint]
        else:
            self.hidden_constraints.append(hidden_constraint)

        if filter_data:
            self._filter_hidden_constraints()

    def add_hidden_constraints(self, hidden_constraints):
        """Add multiple hidden constraints to the surrogate model

        :param hidden_constraints: HiddenConstraint
        """

        for hidden_constraint in hidden_constraints:
            self.add_hidden_constraint(hidden_constraint, filter_data=False)

    def predict(self, input_data: dict):
        """Predict the model's response for a set of input data.

        Input data has the format of ['varName': data, 'varName2': data2]. Return has identical structure.

        :param input_data: inputs for prediction
        :type input_data: dict
        :return: predicted response
        :rtype: dict
        """

        assert self.surrogate is not None, "Surrogate model is not trained. Call surrogate_model.build() first."

        predict_input = np.zeros((1, self.input_dimension))

        # Ensure order of input vector is correct
        for variable, index in self.input_samples_indices.items():
            predict_input[0, index] = input_data[variable]

        predict_output = self.surrogate.predict_values(predict_input)

        output = dict()
        for variable, index in self.output_samples_indices.items():
            output[variable] = predict_output[0, index]

        # Check if hidden constraints are violated.
        if len(self.hidden_constraints) > 0:
            violated_hidden_constraint = self.hidden_constraints_predictor.predict(predict_input)
        else:
            violated_hidden_constraint = False

        # And handle the violated hidden constraints if necessary
        if violated_hidden_constraint:
            for hidden_constraint in self.hidden_constraints:
                for action in hidden_constraint.actions:
                    if action['action'] == 'set_variable_to_value':
                        for action_var in action:
                            if action_var == 'action':
                                continue
                            else:
                                output[action_var] = action[action_var]

                    elif action['action'] == 'set_variables_to_value_from_file':
                        file = action['filename']
                        assert os.path.isfile(file), "Provided file for resetting the variables to file is not valid."

                        reset_data = PortableCpacs(file)
                        for variable_to_reset in action['variables_to_reset']:
                            output[variable_to_reset] = reset_data.get_value(variable_to_reset)

        return output

    def propose_new_sample_grid(self, combine=True, method='EIGF', n_samples=None, seed=None):
        variable_bounds = self.input_bounds
        limits = np.zeros((self.input_dimension, 2))

        for var in variable_bounds:
            idx = self.input_samples_indices[var]
            limits[idx, 0] = variable_bounds[var][0]
            limits[idx, 1] = variable_bounds[var][1]

        smt_doe = smt_sampling.LHS(xlimits=limits, random_state=seed)
        smt_samples = smt_doe(n_samples)

        hidden_constraints = self.hidden_constraints_predictor.predict(X=smt_samples)
        smt_samples = smt_samples[hidden_constraints[:] == False, :]

        RC = np.apply_along_axis(lambda x: self._calculate_refinement_grid(x=x.reshape(1, -1), method=method),
                                 axis=-1, arr=smt_samples)
        RC = np.squeeze(RC, axis=1)

        max_RC_samples = np.argmax(RC, axis=0)

        output_sample = {}
        idx_included = []
        for output_var, idx_out in self.output_samples_indices.items():
            if idx_out in self.hidden_constraint_output_indices:
                continue
            if max_RC_samples[idx_out] in idx_included:
                print('Sample already in proposed samples')
                continue

            if not combine:
                output_sample[output_var] = dict()

            selected_sample = smt_samples[max_RC_samples[idx_out], :]

            for input_var, idx_in in self.input_samples_indices.items():
                if combine:
                    if input_var not in output_sample:
                        output_sample[input_var] = [selected_sample[idx_in]]
                    else:
                        output_sample[input_var].append(selected_sample[idx_in])
                else:
                    output_sample[output_var][input_var] = selected_sample[idx_in]

            idx_included.append(max_RC_samples[idx_out])

        return output_sample

    def propose_new_sample(self, combine=True, method='EIGF'):
        random_start = random.randint(0, self.n_valid_samples - 1)
        x0_full = self.valid_input_samples[random_start, :]

        normalized_input_samples = np.divide(self.valid_input_samples, x0_full)
        normalized_input_samples = np.nan_to_num(normalized_input_samples)
        norm_bounds = []
        x0 = []
        design_variables = self.get_design_variables()
        design_var_indices = {}
        for var, idx in self.input_samples_indices.items():
            if var in design_variables:
                x0.append(x0_full[idx])
                design_var_indices[var] = len(x0) - 1
                norm_bounds.append((min(normalized_input_samples[:, idx]), max(normalized_input_samples[:, idx])))

        x0 = np.array(x0)
        x0_norm = np.divide(x0, x0)
        y0 = self.valid_output_samples[random_start, :]

        const_func = lambda x_const: self._refinement_criterion_constraint(x_const, x0, x0_full, design_var_indices)

        if self.hidden_constraint_output_indices:
            eq_const = {'type': 'ineq',
                        'fun': const_func}
        else:
            eq_const = None

        cons = []

        for factor in range(len(norm_bounds)):
            lower, upper = norm_bounds[factor]
            l = {'type': 'ineq',
                 'fun': lambda x, lb=lower, i=factor: x[i] - lb}
            u = {'type': 'ineq',
                 'fun': lambda x, ub=upper, i=factor: ub - x[i]}
            cons.append(l)
            cons.append(u)

        if eq_const is not None:
            cons.append(eq_const)

        optimizer_args = dict(constraints=cons, method='COBYLA',
                              options={'disp': True, 'rhobeg': 0.2})

        # result = scipy.optimize.minimize(obj_func, x0_norm, constraints=cons, method='COBYLA',
        #                                 options={'disp': True, 'rhobeg': 0.2})

        # result = scipy.optimize.minimize(obj_func, x0_norm, **optimizer_args)
        # result = scipy.optimize.basinhopping(obj_func, x0_norm, stepsize=0.2, niter=10, minimizer_kwargs=optimizer_args)
        proposed_samples = {}
        for output_variable in self.output_variables:
            obj_func = lambda x_obj: self._calculate_refinement_criterion(x_obj, x0, x0_full, design_var_indices,
                                                                          output_variable, method=method)
            result = scipy.optimize.shgo(func=obj_func, bounds=norm_bounds, constraints=cons,
                                         minimizer_kwargs=optimizer_args, options=dict(disp=True))

            x_result = np.multiply(result.x, x0)
            x_full = self.rebuild_vector_from_design_variables(x_result, x0_full, design_var_indices)

            if not combine:
                output_sample = {}
                for var, idx in self.input_samples_indices.items():
                    output_sample[var] = x_full[0, idx]

                proposed_samples[output_variable] = output_sample
            else:
                for var, idx in self.input_samples_indices.items():
                    if var in proposed_samples:
                        proposed_samples[var].append(x_full[0, idx])
                    else:
                        proposed_samples[var] = [x_full[0, idx]]

        return proposed_samples

    def rebuild_vector_from_design_variables(self, x_des, x_full, design_var_indices):
        x = np.zeros(x_full.shape)

        for var, idx in self.input_samples_indices.items():
            if var in design_var_indices:
                design_var_idx = design_var_indices[var]
                x[idx] = x_des[design_var_idx]
            else:
                x[idx] = x_full[idx]

        return x.reshape(1, -1)

    def rebuild_matrix_from_design_variables(self, x_des, design_var_indices, x_full=None):
        if x_full is None:
            x_full = self.valid_input_samples

        x = np.zeros((x_des.shape[0], x_full.shape[1]))

        for var, idx in self.input_samples_indices.items():
            if var in design_var_indices:
                design_var_idx = design_var_indices[var]
                x[:, idx] = x_des[:, design_var_idx]
            else:
                x[:, idx] = x_full[0, idx]

        return x

    def _refinement_criterion_constraint(self, x_norm_des, x0, x0_full, design_var_indices):
        x_des = np.multiply(x_norm_des, x0)

        x = self.rebuild_vector_from_design_variables(x_des=x_des, x_full=x0_full,
                                                      design_var_indices=design_var_indices)
        violated_hidden_constraints = self.hidden_constraints_predictor.predict(X=x)
        if violated_hidden_constraints:
            return int(-10)
        else:
            return int(10)

    def _calculate_refinement_grid(self, x, method='EIGF'):
        x_norm = np.divide(x, x)
        input_samples_norm = np.divide(self.valid_input_samples, x)
        dist = np.linalg.norm(input_samples_norm - x_norm, axis=1)

        closest_idx = np.argmin(dist)
        closest_res = self.valid_output_samples[closest_idx, :]

        predicted_output = self.surrogate.predict_values(x=x)
        predicted_variance = self.surrogate.predict_variances(x=x)

        if method == 'EIGF':
            RC = np.power(predicted_output - closest_res, 2) + predicted_variance
        elif method == 'Variance':
            RC = predicted_variance
        else:
            raise AssertionError('Please provide a valid refinement metric')

        return RC

    def _calculate_refinement_criterion(self, x_norm_des, x0, x0_full, design_var_indices, output_variable,
                                        method='EIGF'):
        x_des = np.multiply(x_norm_des, x0)

        x = self.rebuild_vector_from_design_variables(x_des=x_des, x_full=x0_full,
                                                      design_var_indices=design_var_indices)

        x_norm = np.divide(x, x0_full)

        input_samples_norm = np.divide(self.valid_input_samples, x0_full)
        output_samples = self.valid_output_samples

        dist = np.linalg.norm(input_samples_norm - x_norm, axis=1)
        closest_idx = np.argmin(dist)
        closest_res = output_samples[closest_idx, :]

        predicted_output = self.surrogate.predict_values(x=x)
        predicted_variance = self.surrogate.predict_variances(x=x)

        if method == 'EIGF':
            RC = np.power(predicted_output - closest_res, 2) + predicted_variance
        elif method == 'Variance':
            RC = predicted_variance
        return -1 * RC[0, self.output_samples_indices[output_variable]]

    def write_cpacs_response(self, cpacs_in: str, cpacs_out: str):
        cpacs = PortableCpacs(cpacs_in)

        cpacs_input = {}
        for variable in self.input_variables:
            cpacs_input[variable] = cpacs.get_value(variable)

        response = self.predict(cpacs_input)

        for variable in self.output_variables:
            cpacs.update_value(variable, response[variable])

        cpacs.save(cpacs_out=cpacs_out)

    def train_hidden_constraints(self):
        """Train a random forest to model the hidden constraints in the discipline

        :return:
        """
        samples_violating_constraints = []
        for idx_sample in range(0, self.n_total_samples):
            samples_violating_constraints.append(idx_sample in self.invalid_samples)

        random_forest = RandomForestClassifier(n_jobs=-1, oob_score=True)
        random_forest.fit(self.all_input_samples, samples_violating_constraints)

        self.hidden_constraints_predictor = random_forest

    @property
    def input_dimension(self):
        if self.input_data:
            return len(self.input_data)
        else:
            return None

    @property
    def output_dimension(self):
        if self.output_data:
            return len(self.output_data)
        else:
            return None

    @property
    def input_bounds(self):
        bounds = {}
        for input_var, idx in self.input_samples_indices.items():
            bounds[input_var] = [min(self.valid_input_samples[:, idx]), max(self.valid_input_samples[:, idx])]

        return bounds

    def write_text_report(self):
        report_text = f'Trained for {self.n_total_samples} samples, of which {self.n_valid_samples} are valid. \n\r'
        report_text += f'Hidden constraints active: {len(self.hidden_constraints)}. \n\r'

        if self.error is not None:
            report_text += f'Calculated error: {self.error} ({self.validation_metric}). \n\r'
        else:
            report_text += 'No error is calculated. \n\r\n\r'

        output_table = prettytable.PrettyTable(["Variable", "Lower Bound", "Upper Bound"])

        for var, idx in self.input_samples_indices.items():
            output_table.add_row([var, min(self.valid_input_samples[:, idx]), max(self.valid_input_samples[:, idx])])

        report_text += 'Trained for the following variables and their bounds: \n\r'
        report_text += output_table.get_string()

        return report_text

    def _auto_select_surrogate(self, selection, training_set):
        if selection == 'all':
            candidates = self.available_models
        elif isinstance(selection, list):
            candidates = {candidate: self.available_models[candidate] for candidate in selection}
        elif isinstance(selection, str):
            candidates = self.available_models[selection]
        else:
            raise AssertionError('Please provide a valid selection of candidates')

        # Evaluate all candidates
        results = self._evaluate_candidates(candidates)

        # Process results and find optimal model
        opt_config = None
        for result in results:
            if not opt_config:
                opt_config = result
            elif 'error' in result and result['error'] < opt_config['error']:
                opt_config = result

        # Extract configuration, and build model
        opt_config_type = opt_config['model']
        config = {}

        for hyperpar in self.available_models[opt_config_type]['hyperparameters']:
            config[hyperpar] = opt_config[hyperpar]

        error = opt_config['error']
        sur_model = self.available_models[opt_config_type]['function'](**config)

        self.selected_model = opt_config_type
        self.selected_config = config

        # Now train the generated model
        self._train_fixed_type(model_type=opt_config_type, training_set=training_set, sur_model=sur_model, error=error)

    def _train_fixed_type(self, model_type, training_set, k_folds=5, sur_model: smt = None, error: float = None,
                          model_options=None):
        if sur_model:
            self.surrogate = sur_model
        else:
            if not model_options:
                self.surrogate = self.available_models[model_type]['function']()
            else:
                self.surrogate = self.available_models[model_type]['function'](**model_options)

        if training_set == 'full':  # Use all data for SM
            self.surrogate.set_training_values(xt=self.valid_input_samples,
                                               yt=self.valid_output_samples)
            self.error = None
        # Use k-fold validation for model and use last dataset
        elif training_set == 'validated':
            if error is None:
                self.error = self.validate(self.surrogate, method='k-fold', k_folds=k_folds)
            else:
                self.error = error

            # Select samples of last 4 folds, assemble training data and set training values
            # folds = self._get_k_folds(k_folds=k_folds)
            # training_folds = range(1, k_folds)
            # samples_list_in = [folds[idx_fold]['input'] for idx_fold in training_folds]
            # samples_list_out = [folds[idx_fold]['output'] for idx_fold in training_folds]
            # training_samples_in = np.concatenate(samples_list_in, axis=0)
            # training_samples_out = np.concatenate(samples_list_out, axis=0)

            # self.surrogate.set_training_values(xt=training_samples_in,
            #                                   yt=training_samples_out)

            self.surrogate.set_training_values(xt=self.valid_input_samples,
                                               yt=self.valid_output_samples)

        else:
            raise AssertionError("Please provide valid training_set: 'full' or 'validated'")

        self.surrogate.train()

    def _calculate_non_linearity(self):
        """Calculate the non-linearity of the sample set using the Pearson correlation coefficient.

        Non-linearity = 1 - (cov(y_hat, y))/(sqrt(var(y_hat))sqrt(var(y)))

        Where y_hat is the predicted response when a linear model is fitted to the data. Full explanation in the work
        of Jia et al. (2020):

        Liangyue Jia, Reza Alizadeh, Jia Hao, Guoxin Wang, Janet K. Allen, and Farrokh Mistree. A rulebased
        method for automated surrogate model selection. Advanced Engineering Informatics, 45(April):
        101123–101123, 2020. doi: 10.1016/j.aei.2020.101123. URL https://doi.org/10.1016/j.aei.
        2020.101123

        """
        lin_model = smt.LS(print_global=False)
        lin_model.set_training_values(xt=self.valid_input_samples,
                                      yt=self.valid_output_samples)
        lin_model.train()

        lin_prediction = lin_model.predict_values(self.valid_input_samples)

        # Numpy combines the correlation coefficient in a matrix. it has the format:
        #            y_hat_1, ..., y_hat_N , y_1, ..., y_N
        # -------------------------------------
        # y_hat_1   |
        # ...       |
        # y_hat_N   |
        # y_1       |
        # ...       |
        # y_N       |

        corr_coef_matrix = np.corrcoef(lin_prediction, self.valid_output_samples, rowvar=False)
        self.output_non_linearity = {}
        for var in self.output_samples_indices:
            idx = self.output_samples_indices[var]
            i = idx
            j = self.output_dimension + idx
            self.output_non_linearity[var] = 1 - corr_coef_matrix[i, j]  # Closer to 1: higher non-linearity

    def _filter_hidden_constraints(self):
        if self.n_total_samples == 0 or not self.hidden_constraints:
            self.valid_input_samples = self.all_input_samples
            self.valid_output_samples = self.all_output_samples
            self.n_valid_samples = len(self.valid_input_samples)
            self.invalid_samples = []
            return

        self.hidden_constraint_input_indices = []
        self.hidden_constraint_output_indices = []

        for hidden_constraint in self.hidden_constraints:
            # We will need to keep track for this when training and evaluating models. Only if data needs to be ignored
            if not hidden_constraint.ignore_in_training:
                continue
            for flag in hidden_constraint.flags:
                if flag in self.input_samples_indices:
                    self.hidden_constraint_input_indices.append(self.input_samples_indices[flag])
                if flag in self.output_samples_indices:
                    self.hidden_constraint_output_indices.append(self.output_samples_indices[flag])

        valid_samples = []
        invalid_samples = []
        for idx_sample in range(0, self.n_total_samples):
            valid = False
            for hidden_constraint in self.hidden_constraints:
                if not hidden_constraint.ignore_in_training:
                    valid = True
                    continue
                for flag in hidden_constraint.flags:
                    if flag in self.input_samples_indices:
                        sample_flag_val = self.all_input_samples[idx_sample, self.input_samples_indices[flag]]
                    elif flag in self.output_samples_indices:
                        sample_flag_val = self.all_output_samples[idx_sample, self.output_samples_indices[flag]]
                    else:
                        valid = True
                        continue

                    if sample_flag_val == hidden_constraint.flags[flag]:
                        valid = False
                        break
                    else:
                        valid = True

                if not valid:
                    break

            if valid:
                valid_samples.append(idx_sample)
            else:
                invalid_samples.append(idx_sample)

        self.valid_input_samples = np.take(self.all_input_samples, valid_samples, 0)
        self.valid_output_samples = np.take(self.all_output_samples, valid_samples, 0)
        self.n_valid_samples = len(valid_samples)
        self.invalid_samples = invalid_samples

    def _evaluate_candidates(self, candidates, parallel=True):
        """Evaluate a set of candidates.

        IMPORTANT: When using parallel processing in this function, make sure that the script starting the workflow is
        executed from a if __name__ == '__main__' clause. If Random behaviour is occuring (software starting over, etc.)
        this is the solution.

        :param candidates: dict of candidates
        :param parallel: Use parallel processing for candidate evaluation
        :return: dict of results
        """
        possible_configs = self._prepare_possible_configurations(candidates)

        if parallel:
            pool = multiprocessing.Pool()
            results = pool.map(self._optimize_hyperpar_config, possible_configs)
        else:
            results = []
            for possible_config in possible_configs:
                results.append(self._optimize_hyperpar_config(possible_config))
        return results

    def _optimize_hyperpar_config(self, hyperpars):
        sur_model_type = hyperpars['model']
        sur_model_func = self.available_models[sur_model_type]['function']

        result = dict(model=sur_model_type)

        hyperpars = self._optim_hyperpar(sur_model_func,
                                         current_hyperpars=hyperpars['discrete'],
                                         continuous_hyperpars=hyperpars['continuous'])

        if hyperpars:
            result.update(hyperpars)
        return result

    # Build objective function for hyperparameter set; retrieve RMSE for sur_model config
    def _evaluate_hyperpars(self, sur_func, current_hyperpars, continuous_hyperpars, x):
        try:
            for idx, hyperpar in enumerate(continuous_hyperpars):
                current_hyperpars[hyperpar] = x[idx]

            sur_model = sur_func(**current_hyperpars)

            score = self.validate(sur_model=sur_model, method='k-fold', k_folds=5)
            return score
        except:
            print('Something went wrong. Returning abnormally high value')
            return 1E10

    def _optim_hyperpar(self, sur_func, current_hyperpars, continuous_hyperpars):
        if len(continuous_hyperpars) == 0:
            # No more discrete and no continuous parameters to vary. Validate this config and return
            try:
                config = current_hyperpars.copy()
                config['error'] = self.validate(sur_func(**current_hyperpars), method='k-fold', k_folds=5)
                return config
            except Exception as e:
                print(f'Parameters gave an error. Non-valid hyperparameters. Exception: {e}')
                # traceback.print_exc()
                return None
        else:
            # No discrete hyperparameters anymore to vary.
            # Bundle the continuous variables, create an objective function and use scipy's optimizer to optimize
            x0 = []
            bounds = []
            for par in continuous_hyperpars:
                x0.append(continuous_hyperpars[par]['nom'])
                bounds.append((continuous_hyperpars[par]['min'], continuous_hyperpars[par]['max']))

            obj = lambda x: self._evaluate_hyperpars(sur_func=sur_func, current_hyperpars=current_hyperpars,
                                                     continuous_hyperpars=continuous_hyperpars, x=x)

            min = scipy.optimize.minimize(obj, x0=np.array(x0), bounds=bounds)

            if not min['success']:
                print('Something went wrong for this parameter set. Discard')
                return None

            hyperpar_config_result = current_hyperpars.copy()
            for idx, par in enumerate(continuous_hyperpars):
                hyperpar_config_result[par] = min['x'][idx]

            hyperpar_config_result['error'] = min['fun']

            return hyperpar_config_result

    def _process_data(self):
        """Prepare incoming data for the SMT format (numpy).
        """
        # Store in SMT specific format
        self.all_input_samples = np.zeros((self.n_total_samples, self.input_dimension))
        self.input_samples_indices = dict()
        for index, variable in enumerate(self.input_data):
            assert len(
                self.input_data[variable]) == self.n_total_samples, "Sample size should be equal for all variables"

            self.input_samples_indices[variable] = index
            self.all_input_samples[:, index] = self.input_data[variable]

        self.all_output_samples = np.zeros((self.n_total_samples, self.output_dimension))
        self.output_samples_indices = dict()
        for index, variable in enumerate(self.output_data):
            assert len(
                self.output_data[variable]) == self.n_total_samples, "Sample size should be equal for all variables"

            self.output_samples_indices[variable] = index
            self.all_output_samples[:, index] = self.output_data[variable]

    def _find_in_and_output_providers(self):
        """Find input and output providers from the provided in- and output variables.

        This step is necessary for the acquisition of the samples from the individual disciplines
        """
        input_provider = dict()
        output_provider = dict()
        for discipline in self.disciplines:
            if len(input_provider) == 0 and len(output_provider) == 0:
                for input_var in discipline.input_variables:
                    if input_var in self.input_variables:
                        input_provider[input_var] = discipline
                for output_var in discipline.output_variables:
                    if output_var in self.output_variables:
                        output_provider[output_var] = discipline
            else:
                input_variables = discipline.input_variables
                for input_var in input_variables:
                    if input_var in self.input_variables and input_var not in input_provider:
                        input_provider[input_var] = discipline

                output_variables = discipline.output_variables
                for output_var in output_variables:
                    if output_var in self.output_variables:
                        output_provider[output_var] = discipline  # Last output provider is used

        assert len(input_provider) == len(self.input_variables), 'Some input variables have no provider'
        assert len(output_provider) == len(self.output_variables), 'Some output variables have no provider'

        self.input_providers = input_provider
        self.output_providers = output_provider

    def _split_sample_validation(self, sur_model: smt, **kwargs):
        """Execute a split_sample validation for a SM

        :param sur_model: SMT object of a surrogate model
        :param kwargs: split_size: relative size of training set to testing sets. Defaults to 0.7
        :return: list of RMSE's for the different folds
        """
        if 'split_size' in kwargs:
            split_size = kwargs['split_size']
        else:
            split_size = 0.7

        sample_numbers_shuffled = list(range(0, self.n_valid_samples))
        random.shuffle(sample_numbers_shuffled)

        training_idx = sample_numbers_shuffled[0:math.floor(split_size * self.n_valid_samples)]
        test_idx = sample_numbers_shuffled[math.floor(split_size * self.n_valid_samples):]

        training_samples_in = np.take(self.valid_input_samples, training_idx, 0)
        training_samples_out = np.take(self.valid_output_samples, training_idx, 0)

        test_samples_in = np.take(self.valid_input_samples, test_idx, 0)
        test_samples_out = np.take(self.valid_output_samples, test_idx, 0)

        error = self._get_surrogate_error(sur_model=sur_model,
                                          training_samples_in=training_samples_in,
                                          training_samples_out=training_samples_out,
                                          test_samples_in=test_samples_in,
                                          test_samples_out=test_samples_out)

        return error

    def _get_k_folds(self, k_folds=5, seed=1):
        """ Generate k_folds for current dataset

        :param k_folds: Amount of folds to generate
        :param seed: Random number seed to ensure consistency in fold generation
        :return: dict with k_folds
        """
        sample_numbers_shuffled = list(range(0, self.n_valid_samples))
        random.Random(seed).shuffle(sample_numbers_shuffled)

        fold_base_size = math.floor(self.n_valid_samples / k_folds)
        folds = list()
        for idx_fold in range(0, k_folds):
            fold_size = fold_base_size
            if idx_fold >= k_folds - (self.n_valid_samples % k_folds):  # Divide left-over samples over last folds
                fold_size += 1

            idx_samples = []
            for idx_fold_sample in range(0, fold_size):
                idx_samples.append(sample_numbers_shuffled.pop(0))
            folds.append({})
            folds[idx_fold]['input'] = np.take(self.valid_input_samples, idx_samples, 0)
            folds[idx_fold]['output'] = np.take(self.valid_output_samples, idx_samples, 0)

        return folds

    def _k_fold_validation(self, sur_model: smt, seed=1, **kwargs):
        """Execute a K-fold validation for certain surrogate model

        :param sur_model: SMT object of a surrogate model
        :param kwargs: k_folds: amount of folds to make
        :return: list of RMSE's for the different folds
        """
        # Prepare amount of folds: k_folds
        if 'k_folds' in kwargs:
            k_folds = kwargs['k_folds']
        else:
            k_folds = 5

        if k_folds > self.n_valid_samples:
            print('Requested k for k-folds validation higher than amount of samples. Setting k=n')
            k_folds = self.n_valid_samples

        folds = self._get_k_folds(k_folds=k_folds, seed=seed)

        # Actually execute the k-Fold validation procedure
        fold_indices = range(0, k_folds)
        errors = []
        for test_fold in fold_indices:
            # Divide and extract samples from folds  TODO: Can be made with less copying etc. Direct indexing of samples
            test_samples_in = folds[test_fold]['input']
            test_samples_out = folds[test_fold]['output']

            samples_list_in = [folds[idx_fold]['input'] for idx_fold in fold_indices if not idx_fold == test_fold]
            samples_list_out = [folds[idx_fold]['output'] for idx_fold in fold_indices if not idx_fold == test_fold]

            training_samples_in = np.concatenate(samples_list_in, axis=0)
            training_samples_out = np.concatenate(samples_list_out, axis=0)

            errors.append(self._get_surrogate_error(sur_model=sur_model,
                                                    training_samples_in=training_samples_in,
                                                    training_samples_out=training_samples_out,
                                                    test_samples_in=test_samples_in,
                                                    test_samples_out=test_samples_out))

        return errors

    def _get_surrogate_error(self, sur_model: smt, training_samples_in: np.ndarray, training_samples_out: np.ndarray,
                             test_samples_in: np.ndarray, test_samples_out: np.ndarray):
        """ Validate a surrogate model for a set of training and testing data

        :param sur_model: SMT object of a surrogate model
        :param training_samples_in: ndarray of training samples input
        :param training_samples_out: ndarray of training samples output
        :param test_samples_in: ndarray of testing samples input
        :param test_samples_out: ndarray of testing samples output
        :return: RMSE of prediction to test samples
        """
        sur_model.set_training_values(xt=training_samples_in,
                                      yt=training_samples_out)

        sur_model.train()
        prediction = sur_model.predict_values(test_samples_in)

        # If hidden constraints are present in the model output, remove them for validating purposes
        if self.hidden_constraint_output_indices:
            prediction = np.delete(prediction, obj=self.hidden_constraint_output_indices, axis=1)
            test_samples_out = np.delete(test_samples_out, obj=self.hidden_constraint_output_indices, axis=1)
            training_samples_out = np.delete(training_samples_out, obj=self.hidden_constraint_output_indices, axis=1)

        difference = prediction - test_samples_out

        if self.validation_metric == 'RMSPE':
            difference = difference / test_samples_out
            error = np.mean(np.sqrt(np.sum(difference ** 2, axis=0) / test_samples_out.shape[0]))
        elif self.validation_metric == 'RMSE':
            error = np.mean(np.sqrt(np.sum(difference ** 2, axis=0) / test_samples_out.shape[0]))
        elif self.validation_metric == 'NRMSE':
            rmse = np.sqrt(np.sum(difference ** 2, axis=0) / test_samples_out.shape[0])
            min_training_val = np.amin(training_samples_out, axis=0)
            max_training_val = np.amax(training_samples_out, axis=0)
            error = np.divide(rmse, max_training_val - min_training_val)
            error = np.mean(error)
        elif self.validation_metric == 'R2':
            numerator = np.sum(difference ** 2)
            denominator = np.sum((prediction - np.mean(training_samples_out, axis=0)) ** 2)
            error = np.mean(1 - numerator / denominator)
        else:
            raise AssertionError('Please provide a valid validation metric!')

        return error

    @staticmethod
    def _prepare_possible_configurations(candidates):
        """Build a dict of all possible configuration in the provided candidates

        :return: dict with configurations
        """

        def find_hyperpar_combinations(configs, current_hyperpars, continuous_hyperpars, discrete_hyperpars):
            if len(discrete_hyperpars) > 0:
                # Still discrete hyperparameters to manage
                discrete_hyperpar = next(iter(discrete_hyperpars))
                discrete_hyperpar_values = discrete_hyperpars[discrete_hyperpar]

                discrete_hyperpars.pop(discrete_hyperpar)

                for value in discrete_hyperpar_values:
                    current_hyperpars = copy.deepcopy(current_hyperpars)
                    current_hyperpars['discrete'][discrete_hyperpar] = value
                    configs = find_hyperpar_combinations(configs,
                                                         current_hyperpars,
                                                         continuous_hyperpars,
                                                         copy.deepcopy(discrete_hyperpars))
            elif len(discrete_hyperpars) == 0 and len(continuous_hyperpars) == 0:
                configs.append(current_hyperpars)
            else:
                # Only continous hyperparameters to vary
                current_hyperpars['continuous'].update(continuous_hyperpars)
                configs.append(current_hyperpars)

            return configs

        configs = []

        for model in candidates:
            print(f'Evaluating {model}')
            model_info = candidates[model]

            discrete_hyperpars = {}
            continuous_hyperpars = {}

            print('Almost at hyperpar opt')
            for hyperpar in model_info['hyperparameters']:
                hyperpar_data = model_info['hyperparameters'][hyperpar]
                print('Reached hyperpar opt')
                if type(hyperpar_data) is dict:
                    continuous_hyperpars[hyperpar] = hyperpar_data
                else:
                    discrete_hyperpars[hyperpar] = hyperpar_data

            configs = find_hyperpar_combinations(configs=configs,
                                                 current_hyperpars=dict(model=model,
                                                                        discrete={},
                                                                        continuous={}),
                                                 continuous_hyperpars=continuous_hyperpars,
                                                 discrete_hyperpars=discrete_hyperpars)

        return configs
